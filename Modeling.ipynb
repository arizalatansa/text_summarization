{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets\n",
    "# !pip install evaluate\n",
    "# !pip install accelerate -U\n",
    "# !pip install transformers[torch]\n",
    "# !pip install rouge\n",
    "# !pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from datasets import Dataset\n",
    "from transformers import BertModel, BertTokenizer, BertConfig, BertForSequenceClassification\n",
    "from transformers import EncoderDecoderConfig, EncoderDecoderModel\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import pipeline\n",
    "from rouge import Rouge\n",
    "from datasets import load_metric\n",
    "from transformers import PretrainedConfig\n",
    "from transformers import BertConfig, EncoderDecoderConfig\n",
    "from transformers import BertPreTrainedModel, BertModel\n",
    "from transformers.models.bart.modeling_bart import BartForConditionalGeneration\n",
    "import transformers\n",
    "from transformers.generation_utils import GenerationMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.35.2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # exctract from specific folder in tar file\n",
    "# # tar -zxvf [tar file path] [directory target extracted data] [directory target on tar file]\n",
    "\n",
    "# # EXTRME DATA\n",
    "# !tar -zxvf /content/drive/MyDrive/liputan6_data.tar.gz C /content/drive/MyDrive/SUMMARIZATION_MODEL_FINE_TUNING/id_liputan6/ liputan6_data/xtrme/\n",
    "# !tar -zxvf /content/drive/MyDrive/liputan6_data.tar.gz C /content/drive/MyDrive/SUMMARIZATION_MODEL_FINE_TUNING/id_liputan6/ liputan6_data/xtrme/test/\n",
    "# !tar -zxvf /content/drive/MyDrive/liputan6_data.tar.gz C /content/drive/MyDrive/SUMMARIZATION_MODEL_FINE_TUNING/id_liputan6/ liputan6_data/xtrme/dev/\n",
    "\n",
    "# # CANONICAL DATA\n",
    "# !tar -zxvf /content/drive/MyDrive/liputan6_data.tar.gz C /content/drive/MyDrive/SUMMARIZATION_MODEL_FINE_TUNING/id_liputan6/ liputan6_data/canonical/\n",
    "# !tar -zxvf /content/drive/MyDrive/liputan6_data.tar.gz C /content/drive/MyDrive/SUMMARIZATION_MODEL_FINE_TUNING/id_liputan6/ liputan6_data/canonical/train/\n",
    "# !tar -zxvf /content/drive/MyDrive/liputan6_data.tar.gz C /content/drive/MyDrive/SUMMARIZATION_MODEL_FINE_TUNING/id_liputan6/ liputan6_data/canonical/test/\n",
    "# !tar -zxvf /content/drive/MyDrive/liputan6_data.tar.gz C /content/drive/MyDrive/SUMMARIZATION_MODEL_FINE_TUNING/id_liputan6/ liputan6_data/canonical/dev/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract JSON Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Data to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./df_train.csv')\n",
    "test_df = pd.read_csv('./df_test.csv')\n",
    "dev_df = pd.read_csv('./df_dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>clean_article</th>\n",
       "      <th>clean_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Liputan6. com, Jakarta : Presiden Susilo Bamba...</td>\n",
       "      <td>Menurut Presiden Susilo Bambang Yudhoyono, kem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Liputan6. com, Jakarta : Perdana Menteri Jepan...</td>\n",
       "      <td>Pada masa silam Jepang terlalu ambisius untuk ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Liputan6. com, Kutai : Banjir dengan ketinggia...</td>\n",
       "      <td>Puluhan hektare areal persawahan yang sebagian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Liputan6. com, Jakarta : Presiden Susilo Bamba...</td>\n",
       "      <td>Sekjen PBB Kofi Annan memuji langkah Presiden ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Liputan6. com, Solok : Warga Kampung Batu Dala...</td>\n",
       "      <td>Untuk mempercepat pelaksanaan belajar-mengajar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                      clean_article  \\\n",
       "0           0  Liputan6. com, Jakarta : Presiden Susilo Bamba...   \n",
       "1           1  Liputan6. com, Jakarta : Perdana Menteri Jepan...   \n",
       "2           2  Liputan6. com, Kutai : Banjir dengan ketinggia...   \n",
       "3           3  Liputan6. com, Jakarta : Presiden Susilo Bamba...   \n",
       "4           4  Liputan6. com, Solok : Warga Kampung Batu Dala...   \n",
       "\n",
       "                                       clean_summary  \n",
       "0  Menurut Presiden Susilo Bambang Yudhoyono, kem...  \n",
       "1  Pada masa silam Jepang terlalu ambisius untuk ...  \n",
       "2  Puluhan hektare areal persawahan yang sebagian...  \n",
       "3  Sekjen PBB Kofi Annan memuji langkah Presiden ...  \n",
       "4  Untuk mempercepat pelaksanaan belajar-mengajar...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>clean_article</th>\n",
       "      <th>clean_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Liputan6. com, Jakarta : Pemerintah masih memb...</td>\n",
       "      <td>Pemerintah memberikan tenggat 14 hari kepada p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Liputan6. com, Jakarta : Diperkirakan 11 juta ...</td>\n",
       "      <td>Satu dari 20 orang Indonesia diperkirakan meng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Liputan6. com, Banda Aceh : Aksi peledakan kan...</td>\n",
       "      <td>Peledakan bom kembali terjadi di Aceh. Kali in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Liputan6. com, Surabaya : Petugas Kepolisian R...</td>\n",
       "      <td>Polres Surabaya Timur menangkap seorang penged...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Liputan6. com, Jakarta : Yogyakarta dan Bali m...</td>\n",
       "      <td>Lima seniman Yogyakarta dan Bali menggelar pam...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                      clean_article  \\\n",
       "0           0  Liputan6. com, Jakarta : Pemerintah masih memb...   \n",
       "1           1  Liputan6. com, Jakarta : Diperkirakan 11 juta ...   \n",
       "2           2  Liputan6. com, Banda Aceh : Aksi peledakan kan...   \n",
       "3           3  Liputan6. com, Surabaya : Petugas Kepolisian R...   \n",
       "4           4  Liputan6. com, Jakarta : Yogyakarta dan Bali m...   \n",
       "\n",
       "                                       clean_summary  \n",
       "0  Pemerintah memberikan tenggat 14 hari kepada p...  \n",
       "1  Satu dari 20 orang Indonesia diperkirakan meng...  \n",
       "2  Peledakan bom kembali terjadi di Aceh. Kali in...  \n",
       "3  Polres Surabaya Timur menangkap seorang penged...  \n",
       "4  Lima seniman Yogyakarta dan Bali menggelar pam...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>clean_article</th>\n",
       "      <th>clean_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Liputan6. com, Jakarta : Kepolisian Daerah Ria...</td>\n",
       "      <td>Kapolda Riau baru Brigjen Pol. Johny Yodjana b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Liputan6. com, Jakarta : Bank Indonesia dinila...</td>\n",
       "      <td>Kendati Bank Sentral AS menurunkan suku bungan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Liputan6. com, Jakarta : Berbagai kendala meng...</td>\n",
       "      <td>Pemerintah bermaksud akan lebih mengandalkan s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Liputan6. com, Jakarta : Penghapusan beberapa ...</td>\n",
       "      <td>Revisi Kepmennaker Nomor 78 Tahun 2001, dinila...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Liputan6. com, Jakarta : Operasi Sadar Jaya ya...</td>\n",
       "      <td>Polisi menangkap 32 pengunjung Diskotik Mileni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                      clean_article  \\\n",
       "0           0  Liputan6. com, Jakarta : Kepolisian Daerah Ria...   \n",
       "1           1  Liputan6. com, Jakarta : Bank Indonesia dinila...   \n",
       "2           2  Liputan6. com, Jakarta : Berbagai kendala meng...   \n",
       "3           3  Liputan6. com, Jakarta : Penghapusan beberapa ...   \n",
       "4           4  Liputan6. com, Jakarta : Operasi Sadar Jaya ya...   \n",
       "\n",
       "                                       clean_summary  \n",
       "0  Kapolda Riau baru Brigjen Pol. Johny Yodjana b...  \n",
       "1  Kendati Bank Sentral AS menurunkan suku bungan...  \n",
       "2  Pemerintah bermaksud akan lebih mengandalkan s...  \n",
       "3  Revisi Kepmennaker Nomor 78 Tahun 2001, dinila...  \n",
       "4  Polisi menangkap 32 pengunjung Diskotik Mileni...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_df.head())\n",
    "\n",
    "display(test_df.head())\n",
    "\n",
    "display(dev_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 193883 entries, 0 to 193882\n",
      "Data columns (total 3 columns):\n",
      " #   Column         Non-Null Count   Dtype \n",
      "---  ------         --------------   ----- \n",
      " 0   Unnamed: 0     193883 non-null  int64 \n",
      " 1   clean_article  193883 non-null  object\n",
      " 2   clean_summary  193883 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 4.4+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10972 entries, 0 to 10971\n",
      "Data columns (total 3 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Unnamed: 0     10972 non-null  int64 \n",
      " 1   clean_article  10972 non-null  object\n",
      " 2   clean_summary  10972 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 257.3+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10972 entries, 0 to 10971\n",
      "Data columns (total 3 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Unnamed: 0     10972 non-null  int64 \n",
      " 1   clean_article  10972 non-null  object\n",
      " 2   clean_summary  10972 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 257.3+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_df.info())\n",
    "print('*'*50)\n",
    "display(test_df.info())\n",
    "print('*'*50)\n",
    "display(dev_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace Data Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset.from_dict(train_df)\n",
    "test_data = Dataset.from_dict(test_df)\n",
    "val_data = Dataset.from_dict(dev_df) # development data for validation in training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load TinyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = \"cahya/bert2bert-indonesian-summarization\" # prajjwal1/bert-tiny #\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
    "bert_tokenizer.bos_token =  bert_tokenizer.cls_token\n",
    "bert_tokenizer.eos_token =  bert_tokenizer.sep_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, EncoderDecoderConfig, EncoderDecoderModel\n",
    "\n",
    "config_encoder = BertConfig(\n",
    "    vocab_size = len(bert_tokenizer.get_vocab()), # sesuaikan dengan vocab tokenizer\n",
    "    hidden_size = 36,\n",
    "    num_hidden_layers = 6,\n",
    "    num_attention_heads = 6,\n",
    "    intermediate_size = 768,\n",
    ")\n",
    "config_decoder = BertConfig(\n",
    "    vocab_size = len(bert_tokenizer.get_vocab()), # sesuaikan dengan vocab tokenizer\n",
    "    hidden_size = 36,\n",
    "    num_hidden_layers = 6,\n",
    "    num_attention_heads = 6,\n",
    "    intermediate_size = 768,\n",
    ")\n",
    "\n",
    "config = EncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)\n",
    "model = EncoderDecoderModel(config=config)\n",
    "model.config.decoder_start_token_id = bert_tokenizer.cls_token_id\n",
    "model.config.pad_token_id = bert_tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[CLS]', '[SEP]', 3, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.bos_token, bert_tokenizer.eos_token, bert_tokenizer.get_vocab()[bert_tokenizer.bos_token], bert_tokenizer.get_vocab()[bert_tokenizer.eos_token]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Feaures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature(batch):\n",
    "  encodings = bert_tokenizer(batch['clean_article'], text_target=batch['clean_summary'],\n",
    "                        max_length=512, truncation=True)\n",
    "\n",
    "  encodings = {'input_ids': encodings['input_ids'],\n",
    "               'attention_mask': encodings['attention_mask'],\n",
    "               'labels': encodings['labels']}\n",
    "\n",
    "  return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = bert_tokenizer.bos_token + \" \"\n",
    "posfix = \" \" + bert_tokenizer.eos_token\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + str(doc) + posfix for doc in examples[\"clean_article\"]]\n",
    "    model_inputs = bert_tokenizer(inputs, max_length=512, truncation=True)\n",
    "    labels = bert_tokenizer(text=examples[\"clean_summary\"], max_length=128, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 193883/193883 [30:56<00:00, 104.42 examples/s]\n",
      "Map: 100%|██████████| 10972/10972 [02:02<00:00, 89.81 examples/s]\n",
      "Map: 100%|██████████| 10972/10972 [01:34<00:00, 115.80 examples/s]\n"
     ]
    }
   ],
   "source": [
    "data_pt_train = train_data.map(preprocess_function, batched=True)\n",
    "data_pt_test = test_data.map(preprocess_function, batched=True)\n",
    "data_pt_val = val_data.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Unnamed: 0', 'clean_article', 'clean_summary', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 193883\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Unnamed: 0', 'clean_article', 'clean_summary', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 10972\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Unnamed: 0', 'clean_article', 'clean_summary', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 10972\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(data_pt_train)\n",
    "display(data_pt_test)\n",
    "display(data_pt_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3,\n",
       " 3,\n",
       " 17715,\n",
       " 1050,\n",
       " 17,\n",
       " 3036,\n",
       " 15,\n",
       " 2647,\n",
       " 29,\n",
       " 2601,\n",
       " 14977,\n",
       " 9183,\n",
       " 15137,\n",
       " 8220,\n",
       " 1738,\n",
       " 8511,\n",
       " 2997,\n",
       " 1510,\n",
       " 13339,\n",
       " 2667,\n",
       " 16,\n",
       " 2667,\n",
       " 3065,\n",
       " 1509,\n",
       " 3549,\n",
       " 1793,\n",
       " 1542,\n",
       " 1566,\n",
       " 3212,\n",
       " 11365,\n",
       " 1510,\n",
       " 2059,\n",
       " 5576,\n",
       " 17,\n",
       " 15137,\n",
       " 8711,\n",
       " 3212,\n",
       " 1542,\n",
       " 1637,\n",
       " 16770,\n",
       " 2814,\n",
       " 1570,\n",
       " 5885,\n",
       " 2855,\n",
       " 2230,\n",
       " 3065,\n",
       " 16,\n",
       " 3549,\n",
       " 17,\n",
       " 3290,\n",
       " 9537,\n",
       " 15137,\n",
       " 1793,\n",
       " 4478,\n",
       " 22684,\n",
       " 3065,\n",
       " 16,\n",
       " 3549,\n",
       " 1495,\n",
       " 2647,\n",
       " 17527,\n",
       " 11399,\n",
       " 15,\n",
       " 2647,\n",
       " 15,\n",
       " 9002,\n",
       " 11,\n",
       " 3082,\n",
       " 18,\n",
       " 23,\n",
       " 12,\n",
       " 34,\n",
       " 12287,\n",
       " 29,\n",
       " 2601,\n",
       " 15137,\n",
       " 2771,\n",
       " 4478,\n",
       " 1814,\n",
       " 1007,\n",
       " 36,\n",
       " 17,\n",
       " 1535,\n",
       " 2091,\n",
       " 22699,\n",
       " 15,\n",
       " 15137,\n",
       " 1855,\n",
       " 4417,\n",
       " 1572,\n",
       " 1911,\n",
       " 13323,\n",
       " 1507,\n",
       " 7275,\n",
       " 28131,\n",
       " 2242,\n",
       " 4510,\n",
       " 6011,\n",
       " 1509,\n",
       " 11199,\n",
       " 1495,\n",
       " 31929,\n",
       " 15464,\n",
       " 4121,\n",
       " 14140,\n",
       " 1509,\n",
       " 14043,\n",
       " 15,\n",
       " 3596,\n",
       " 2207,\n",
       " 17,\n",
       " 3475,\n",
       " 15,\n",
       " 15137,\n",
       " 8444,\n",
       " 4517,\n",
       " 1542,\n",
       " 6765,\n",
       " 10338,\n",
       " 10214,\n",
       " 2259,\n",
       " 15,\n",
       " 1920,\n",
       " 4152,\n",
       " 2066,\n",
       " 1572,\n",
       " 9782,\n",
       " 3212,\n",
       " 1510,\n",
       " 13339,\n",
       " 2076,\n",
       " 6482,\n",
       " 17,\n",
       " 2606,\n",
       " 6633,\n",
       " 5885,\n",
       " 1542,\n",
       " 1609,\n",
       " 10061,\n",
       " 5374,\n",
       " 3458,\n",
       " 6276,\n",
       " 2667,\n",
       " 16,\n",
       " 2667,\n",
       " 3040,\n",
       " 3713,\n",
       " 8713,\n",
       " 1010,\n",
       " 1510,\n",
       " 3242,\n",
       " 1600,\n",
       " 5918,\n",
       " 7028,\n",
       " 17,\n",
       " 4530,\n",
       " 6480,\n",
       " 1538,\n",
       " 1566,\n",
       " 2046,\n",
       " 10823,\n",
       " 2861,\n",
       " 1885,\n",
       " 3065,\n",
       " 1509,\n",
       " 3549,\n",
       " 15,\n",
       " 5885,\n",
       " 1542,\n",
       " 13092,\n",
       " 1786,\n",
       " 18004,\n",
       " 2878,\n",
       " 4517,\n",
       " 18419,\n",
       " 17,\n",
       " 22684,\n",
       " 3065,\n",
       " 16,\n",
       " 3549,\n",
       " 1786,\n",
       " 3597,\n",
       " 1977,\n",
       " 1887,\n",
       " 1994,\n",
       " 17,\n",
       " 3059,\n",
       " 1542,\n",
       " 1786,\n",
       " 13660,\n",
       " 1555,\n",
       " 2606,\n",
       " 5860,\n",
       " 1500,\n",
       " 14625,\n",
       " 1499,\n",
       " 1519,\n",
       " 1750,\n",
       " 3786,\n",
       " 15,\n",
       " 2326,\n",
       " 2070,\n",
       " 15,\n",
       " 1975,\n",
       " 19578,\n",
       " 5885,\n",
       " 3065,\n",
       " 3549,\n",
       " 1535,\n",
       " 7425,\n",
       " 17,\n",
       " 2173,\n",
       " 1675,\n",
       " 15,\n",
       " 1855,\n",
       " 4011,\n",
       " 2548,\n",
       " 1844,\n",
       " 1510,\n",
       " 5515,\n",
       " 1495,\n",
       " 6063,\n",
       " 4166,\n",
       " 1814,\n",
       " 1007,\n",
       " 3272,\n",
       " 16611,\n",
       " 8403,\n",
       " 2607,\n",
       " 1510,\n",
       " 1761,\n",
       " 25426,\n",
       " 2570,\n",
       " 1844,\n",
       " 15807,\n",
       " 22987,\n",
       " 15137,\n",
       " 17,\n",
       " 2606,\n",
       " 2839,\n",
       " 3017,\n",
       " 1766,\n",
       " 1542,\n",
       " 3597,\n",
       " 1495,\n",
       " 3302,\n",
       " 3820,\n",
       " 31967,\n",
       " 1010,\n",
       " 2335,\n",
       " 7824,\n",
       " 15,\n",
       " 3589,\n",
       " 5791,\n",
       " 1766,\n",
       " 5659,\n",
       " 15,\n",
       " 2647,\n",
       " 2102,\n",
       " 17,\n",
       " 3059,\n",
       " 1510,\n",
       " 3043,\n",
       " 2044,\n",
       " 4550,\n",
       " 2178,\n",
       " 17,\n",
       " 2784,\n",
       " 7427,\n",
       " 1542,\n",
       " 9602,\n",
       " 1555,\n",
       " 6526,\n",
       " 1509,\n",
       " 16555,\n",
       " 6264,\n",
       " 3242,\n",
       " 24639,\n",
       " 14332,\n",
       " 17,\n",
       " 2570,\n",
       " 22987,\n",
       " 18292,\n",
       " 1896,\n",
       " 1723,\n",
       " 1527,\n",
       " 1723,\n",
       " 2570,\n",
       " 1844,\n",
       " 1533,\n",
       " 3502,\n",
       " 1844,\n",
       " 4417,\n",
       " 1510,\n",
       " 3242,\n",
       " 17,\n",
       " 1803,\n",
       " 7081,\n",
       " 1555,\n",
       " 2606,\n",
       " 5007,\n",
       " 2066,\n",
       " 17,\n",
       " 1570,\n",
       " 11940,\n",
       " 1538,\n",
       " 15,\n",
       " 2570,\n",
       " 22987,\n",
       " 8711,\n",
       " 2826,\n",
       " 6794,\n",
       " 1814,\n",
       " 1007,\n",
       " 3354,\n",
       " 2046,\n",
       " 10823,\n",
       " 2861,\n",
       " 2832,\n",
       " 20797,\n",
       " 1495,\n",
       " 6482,\n",
       " 3065,\n",
       " 1509,\n",
       " 3549,\n",
       " 17,\n",
       " 3475,\n",
       " 2570,\n",
       " 1844,\n",
       " 1509,\n",
       " 1855,\n",
       " 5918,\n",
       " 9261,\n",
       " 21314,\n",
       " 1487,\n",
       " 10141,\n",
       " 1533,\n",
       " 2221,\n",
       " 11560,\n",
       " 10141,\n",
       " 1766,\n",
       " 3214,\n",
       " 3526,\n",
       " 1924,\n",
       " 15,\n",
       " 20515,\n",
       " 1019,\n",
       " 5630,\n",
       " 8291,\n",
       " 15,\n",
       " 1509,\n",
       " 26210,\n",
       " 1032,\n",
       " 5532,\n",
       " 4356,\n",
       " 1008,\n",
       " 17,\n",
       " 7638,\n",
       " 1519,\n",
       " 1788,\n",
       " 23647,\n",
       " 1510,\n",
       " 1821,\n",
       " 1495,\n",
       " 14845,\n",
       " 3216,\n",
       " 1609,\n",
       " 2410,\n",
       " 17,\n",
       " 11,\n",
       " 1907,\n",
       " 1006,\n",
       " 18,\n",
       " 3996,\n",
       " 2705,\n",
       " 3359,\n",
       " 2351,\n",
       " 1509,\n",
       " 13143,\n",
       " 14994,\n",
       " 27136,\n",
       " 12,\n",
       " 17,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pt_train['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pt_train['attention_mask'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3,\n",
       " 2379,\n",
       " 2601,\n",
       " 14977,\n",
       " 9183,\n",
       " 15137,\n",
       " 15,\n",
       " 11365,\n",
       " 3244,\n",
       " 1637,\n",
       " 3212,\n",
       " 7695,\n",
       " 1510,\n",
       " 13339,\n",
       " 2667,\n",
       " 16,\n",
       " 2667,\n",
       " 1495,\n",
       " 3065,\n",
       " 1509,\n",
       " 3549,\n",
       " 17,\n",
       " 6729,\n",
       " 3212,\n",
       " 1542,\n",
       " 1637,\n",
       " 14685,\n",
       " 2814,\n",
       " 1570,\n",
       " 22684,\n",
       " 3065,\n",
       " 16,\n",
       " 3549,\n",
       " 17,\n",
       " 1]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pt_train['labels'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, EncoderDecoderConfig, EncoderDecoderModel\n",
    "\n",
    "config_encoder = BertConfig(\n",
    "    vocab_size = len(bert_tokenizer.get_vocab()), # sesuaikan dengan vocab tokenizer\n",
    "    hidden_size = 4,\n",
    "    num_hidden_layers = 2,\n",
    "    num_attention_heads = 2,\n",
    "    intermediate_size = 768,\n",
    ")\n",
    "config_decoder = BertConfig(\n",
    "    vocab_size = len(bert_tokenizer.get_vocab()), # sesuaikan dengan vocab tokenizer\n",
    "    hidden_size = 4,\n",
    "    num_hidden_layers = 2,\n",
    "    num_attention_heads = 2,\n",
    "    intermediate_size = 768,\n",
    ")\n",
    "\n",
    "config = EncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)\n",
    "model = EncoderDecoderModel(config=config)\n",
    "model.config.decoder_start_token_id = bert_tokenizer.cls_token_id\n",
    "model.config.pad_token_id = bert_tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = bert_tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, bert_tokenizer.pad_token_id)\n",
    "    decoded_labels = bert_tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != bert_tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=bert_tokenizer, model='t5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=32,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=1,\n",
    "    warmup_steps=500,\n",
    "    logging_steps=100,\n",
    "    gradient_accumulation_steps=32,\n",
    "    fp16=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=data_pt_train,\n",
    "    eval_dataset=data_pt_val,\n",
    "    tokenizer=bert_tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6058 [00:00<?, ?it/s]C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\encoder_decoder\\modeling_encoder_decoder.py:619: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\encoder_decoder\\modeling_encoder_decoder.py:639: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "  2%|▏         | 100/6058 [01:53<1:59:32,  1.20s/it]\n",
      "  2%|▏         | 100/6058 [01:53<1:59:32,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.3704, 'learning_rate': 1e-05, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 200/6058 [04:03<1:35:51,  1.02it/s]\n",
      "  3%|▎         | 200/6058 [04:03<1:35:51,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.3664, 'learning_rate': 2e-05, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 300/6058 [05:44<1:44:06,  1.08s/it]\n",
      "  5%|▍         | 300/6058 [05:44<1:44:06,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.3574, 'learning_rate': 3e-05, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 400/6058 [07:24<1:40:31,  1.07s/it]\n",
      "  7%|▋         | 400/6058 [07:24<1:40:31,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.3411, 'learning_rate': 4e-05, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 500/6058 [09:07<1:30:04,  1.03it/s]\n",
      "  8%|▊         | 500/6058 [09:07<1:30:04,  1.03it/s]C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\encoder_decoder\\modeling_encoder_decoder.py:619: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\encoder_decoder\\modeling_encoder_decoder.py:639: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.3186, 'learning_rate': 5e-05, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 600/6058 [10:45<1:20:54,  1.12it/s]\n",
      " 10%|▉         | 600/6058 [10:45<1:20:54,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.2926, 'learning_rate': 4.910039582583664e-05, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 700/6058 [12:23<1:22:53,  1.08it/s]\n",
      " 12%|█▏        | 700/6058 [12:23<1:22:53,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.2663, 'learning_rate': 4.8200791651673265e-05, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 800/6058 [13:56<1:43:11,  1.18s/it]\n",
      " 13%|█▎        | 800/6058 [13:56<1:43:11,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.2391, 'learning_rate': 4.73011874775099e-05, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 900/6058 [15:40<1:38:14,  1.14s/it]\n",
      " 15%|█▍        | 900/6058 [15:40<1:38:14,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.2107, 'learning_rate': 4.640158330334653e-05, 'epoch': 0.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1000/6058 [17:30<1:51:31,  1.32s/it]\n",
      " 17%|█▋        | 1000/6058 [17:30<1:51:31,  1.32s/it]C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\encoder_decoder\\modeling_encoder_decoder.py:619: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\encoder_decoder\\modeling_encoder_decoder.py:639: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.1815, 'learning_rate': 4.5501979129183165e-05, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 1100/6058 [19:41<1:51:46,  1.35s/it]\n",
      " 18%|█▊        | 1100/6058 [19:41<1:51:46,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.1528, 'learning_rate': 4.4602374955019796e-05, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 1200/6058 [21:47<1:46:48,  1.32s/it]\n",
      " 20%|█▉        | 1200/6058 [21:47<1:46:48,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.1237, 'learning_rate': 4.370277078085643e-05, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 1300/6058 [23:59<1:46:53,  1.35s/it]\n",
      " 21%|██▏       | 1300/6058 [23:59<1:46:53,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.0958, 'learning_rate': 4.280316660669306e-05, 'epoch': 0.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 1400/6058 [26:24<2:02:32,  1.58s/it]\n",
      " 23%|██▎       | 1400/6058 [26:24<2:02:32,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.0692, 'learning_rate': 4.190356243252969e-05, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 1500/6058 [28:24<1:19:23,  1.05s/it]\n",
      " 25%|██▍       | 1500/6058 [28:24<1:19:23,  1.05s/it]C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\encoder_decoder\\modeling_encoder_decoder.py:619: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\encoder_decoder\\modeling_encoder_decoder.py:639: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.0411, 'learning_rate': 4.100395825836632e-05, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 1600/6058 [30:12<1:18:30,  1.06s/it]\n",
      " 26%|██▋       | 1600/6058 [30:12<1:18:30,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.0151, 'learning_rate': 4.010435408420296e-05, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 1700/6058 [31:55<1:06:09,  1.10it/s]\n",
      " 28%|██▊       | 1700/6058 [31:55<1:06:09,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.988, 'learning_rate': 3.920474991003958e-05, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 1800/6058 [33:24<1:02:59,  1.13it/s]\n",
      " 30%|██▉       | 1800/6058 [33:24<1:02:59,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.9633, 'learning_rate': 3.830514573587622e-05, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 1900/6058 [35:09<1:16:15,  1.10s/it]\n",
      " 31%|███▏      | 1900/6058 [35:09<1:16:15,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.9391, 'learning_rate': 3.7405541561712845e-05, 'epoch': 0.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2000/6058 [37:10<1:48:45,  1.61s/it]\n",
      " 33%|███▎      | 2000/6058 [37:10<1:48:45,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.9155, 'learning_rate': 3.650593738754948e-05, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\encoder_decoder\\modeling_encoder_decoder.py:619: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\encoder_decoder\\modeling_encoder_decoder.py:639: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      " 35%|███▍      | 2100/6058 [39:26<1:25:32,  1.30s/it]\n",
      " 35%|███▍      | 2100/6058 [39:26<1:25:32,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.8909, 'learning_rate': 3.5606333213386114e-05, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 2200/6058 [41:30<1:14:39,  1.16s/it]\n",
      " 36%|███▋      | 2200/6058 [41:30<1:14:39,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.8682, 'learning_rate': 3.4706729039222745e-05, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 2300/6058 [43:26<1:11:25,  1.14s/it]\n",
      " 38%|███▊      | 2300/6058 [43:26<1:11:25,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.8473, 'learning_rate': 3.3807124865059376e-05, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 2400/6058 [45:24<1:13:02,  1.20s/it]\n",
      " 40%|███▉      | 2400/6058 [45:24<1:13:02,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.8246, 'learning_rate': 3.290752069089601e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 2500/6058 [47:31<1:17:26,  1.31s/it]\n",
      " 41%|████▏     | 2500/6058 [47:31<1:17:26,  1.31s/it]C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\encoder_decoder\\modeling_encoder_decoder.py:619: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\encoder_decoder\\modeling_encoder_decoder.py:639: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.8043, 'learning_rate': 3.200791651673264e-05, 'epoch': 0.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 2600/6058 [49:14<56:45,  1.02it/s]  \n",
      " 43%|████▎     | 2600/6058 [49:14<56:45,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.7809, 'learning_rate': 3.1108312342569276e-05, 'epoch': 0.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 2700/6058 [50:59<1:00:57,  1.09s/it]\n",
      " 45%|████▍     | 2700/6058 [50:59<1:00:57,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.7629, 'learning_rate': 3.0208708168405904e-05, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 2800/6058 [52:45<54:40,  1.01s/it]  \n",
      " 46%|████▌     | 2800/6058 [52:45<54:40,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.7432, 'learning_rate': 2.930910399424254e-05, 'epoch': 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 2900/6058 [54:28<54:45,  1.04s/it]\n",
      " 48%|████▊     | 2900/6058 [54:28<54:45,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.7246, 'learning_rate': 2.8409499820079166e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 3000/6058 [56:13<50:40,  1.01it/s]  \n",
      " 50%|████▉     | 3000/6058 [56:13<50:40,  1.01it/s]C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\encoder_decoder\\modeling_encoder_decoder.py:619: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\encoder_decoder\\modeling_encoder_decoder.py:639: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.7062, 'learning_rate': 2.75098956459158e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 3100/6058 [57:55<48:52,  1.01it/s]\n",
      " 51%|█████     | 3100/6058 [57:55<48:52,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.6878, 'learning_rate': 2.661029147175243e-05, 'epoch': 0.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 3200/6058 [59:41<48:21,  1.02s/it]  \n",
      " 53%|█████▎    | 3200/6058 [59:41<48:21,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.6711, 'learning_rate': 2.5710687297589063e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 3300/6058 [1:01:28<50:27,  1.10s/it]\n",
      " 54%|█████▍    | 3300/6058 [1:01:28<50:27,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.6552, 'learning_rate': 2.4811083123425694e-05, 'epoch': 0.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 3400/6058 [1:03:20<53:35,  1.21s/it]  \n",
      " 56%|█████▌    | 3400/6058 [1:03:20<53:35,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.6389, 'learning_rate': 2.3911478949262326e-05, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 3500/6058 [1:05:36<53:53,  1.26s/it]  \n",
      " 58%|█████▊    | 3500/6058 [1:05:36<53:53,  1.26s/it]C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\encoder_decoder\\modeling_encoder_decoder.py:619: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\encoder_decoder\\modeling_encoder_decoder.py:639: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.6222, 'learning_rate': 2.3011874775098957e-05, 'epoch': 0.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 3600/6058 [1:07:49<51:43,  1.26s/it]  \n",
      " 59%|█████▉    | 3600/6058 [1:07:49<51:43,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.6077, 'learning_rate': 2.2112270600935588e-05, 'epoch': 0.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 3700/6058 [1:09:55<49:20,  1.26s/it]\n",
      " 61%|██████    | 3700/6058 [1:09:55<49:20,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.5947, 'learning_rate': 2.1212666426772222e-05, 'epoch': 0.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 3800/6058 [1:11:54<45:24,  1.21s/it]\n",
      " 63%|██████▎   | 3800/6058 [1:11:54<45:24,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.5814, 'learning_rate': 2.0313062252608853e-05, 'epoch': 0.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 3900/6058 [1:14:02<45:50,  1.27s/it]\n",
      " 64%|██████▍   | 3900/6058 [1:14:02<45:50,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.5648, 'learning_rate': 1.9413458078445485e-05, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 4000/6058 [1:16:10<44:20,  1.29s/it]\n",
      " 66%|██████▌   | 4000/6058 [1:16:10<44:20,  1.29s/it]C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\encoder_decoder\\modeling_encoder_decoder.py:619: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\encoder_decoder\\modeling_encoder_decoder.py:639: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.5545, 'learning_rate': 1.8513853904282116e-05, 'epoch': 0.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 4100/6058 [1:18:24<33:03,  1.01s/it]\n",
      " 68%|██████▊   | 4100/6058 [1:18:24<33:03,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.5432, 'learning_rate': 1.7614249730118747e-05, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 4200/6058 [1:20:19<40:07,  1.30s/it]\n",
      " 69%|██████▉   | 4200/6058 [1:20:19<40:07,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.5342, 'learning_rate': 1.671464555595538e-05, 'epoch': 0.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 4300/6058 [1:22:06<28:39,  1.02it/s]\n",
      " 71%|███████   | 4300/6058 [1:22:06<28:39,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.5193, 'learning_rate': 1.5815041381792013e-05, 'epoch': 0.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 4400/6058 [1:23:56<25:54,  1.07it/s]\n",
      " 73%|███████▎  | 4400/6058 [1:23:56<25:54,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.5092, 'learning_rate': 1.4915437207628644e-05, 'epoch': 0.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 4500/6058 [1:25:38<25:20,  1.02it/s]\n",
      " 74%|███████▍  | 4500/6058 [1:25:38<25:20,  1.02it/s]C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\encoder_decoder\\modeling_encoder_decoder.py:619: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\encoder_decoder\\modeling_encoder_decoder.py:639: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.5018, 'learning_rate': 1.4015833033465275e-05, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 4600/6058 [1:27:18<24:19,  1.00s/it]\n",
      " 76%|███████▌  | 4600/6058 [1:27:18<24:19,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.4947, 'learning_rate': 1.3116228859301908e-05, 'epoch': 0.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 4700/6058 [1:28:58<24:01,  1.06s/it]\n",
      " 78%|███████▊  | 4700/6058 [1:28:58<24:01,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.4826, 'learning_rate': 1.2216624685138539e-05, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 4800/6058 [1:30:44<21:51,  1.04s/it]\n",
      " 79%|███████▉  | 4800/6058 [1:30:44<21:51,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.477, 'learning_rate': 1.1317020510975172e-05, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 4900/6058 [1:32:26<17:40,  1.09it/s]\n",
      " 81%|████████  | 4900/6058 [1:32:26<17:40,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.4686, 'learning_rate': 1.0417416336811803e-05, 'epoch': 0.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5000/6058 [1:33:55<15:47,  1.12it/s]\n",
      " 83%|████████▎ | 5000/6058 [1:33:55<15:47,  1.12it/s]C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\encoder_decoder\\modeling_encoder_decoder.py:619: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\encoder_decoder\\modeling_encoder_decoder.py:639: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.4621, 'learning_rate': 9.517812162648434e-06, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 5100/6058 [1:35:25<14:01,  1.14it/s]\n",
      " 84%|████████▍ | 5100/6058 [1:35:25<14:01,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.454, 'learning_rate': 8.618207988485067e-06, 'epoch': 0.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 5200/6058 [1:36:54<12:15,  1.17it/s]\n",
      " 86%|████████▌ | 5200/6058 [1:36:54<12:15,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.4515, 'learning_rate': 7.718603814321698e-06, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 5300/6058 [1:38:22<10:56,  1.16it/s]\n",
      " 87%|████████▋ | 5300/6058 [1:38:22<10:56,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.4468, 'learning_rate': 6.81899964015833e-06, 'epoch': 0.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 5400/6058 [1:39:51<09:45,  1.12it/s]\n",
      " 89%|████████▉ | 5400/6058 [1:39:51<09:45,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.4406, 'learning_rate': 5.919395465994963e-06, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 5500/6058 [1:41:21<08:01,  1.16it/s]\n",
      " 91%|█████████ | 5500/6058 [1:41:21<08:01,  1.16it/s]C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\encoder_decoder\\modeling_encoder_decoder.py:619: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\encoder_decoder\\modeling_encoder_decoder.py:639: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.4369, 'learning_rate': 5.0197912918315946e-06, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 5600/6058 [1:42:50<06:47,  1.12it/s]\n",
      " 92%|█████████▏| 5600/6058 [1:42:50<06:47,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.4319, 'learning_rate': 4.1201871176682265e-06, 'epoch': 0.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 5700/6058 [1:44:20<05:21,  1.11it/s]\n",
      " 94%|█████████▍| 5700/6058 [1:44:20<05:21,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.4318, 'learning_rate': 3.220582943504858e-06, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 5800/6058 [1:45:48<03:46,  1.14it/s]\n",
      " 96%|█████████▌| 5800/6058 [1:45:48<03:46,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.4292, 'learning_rate': 2.32097876934149e-06, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 5900/6058 [1:47:17<02:21,  1.11it/s]\n",
      " 97%|█████████▋| 5900/6058 [1:47:17<02:21,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.4299, 'learning_rate': 1.4213745951781216e-06, 'epoch': 0.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 6000/6058 [1:48:47<00:51,  1.14it/s]\n",
      " 99%|█████████▉| 6000/6058 [1:48:47<00:51,  1.14it/s]C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\encoder_decoder\\modeling_encoder_decoder.py:619: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\encoder_decoder\\modeling_encoder_decoder.py:639: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.4246, 'learning_rate': 5.217704210147535e-07, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6058/6058 [1:49:39<00:00,  1.05it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                     \n",
      "\n",
      "100%|██████████| 6058/6058 [1:52:32<00:00,  1.05it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 6058/6058 [1:52:32<00:00,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 9.425400733947754, 'eval_runtime': 172.89, 'eval_samples_per_second': 63.462, 'eval_steps_per_second': 1.984, 'epoch': 1.0}\n",
      "{'train_runtime': 6752.8123, 'train_samples_per_second': 28.711, 'train_steps_per_second': 0.897, 'train_loss': 9.77579588715335, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6058, training_loss=9.77579588715335, metrics={'train_runtime': 6752.8123, 'train_samples_per_second': 28.711, 'train_steps_per_second': 0.897, 'train_loss': 9.77579588715335, 'epoch': 1.0})"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = test_data['clean_article'][0]\n",
    "summary = test_data['clean_summary'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_summary(document):\n",
    "  device = model.device\n",
    "  tokenized = bert_tokenizer([document], max_length=512, truncation =True, padding ='longest',return_tensors='pt')\n",
    "  tokenized = {k: v.to(device) for k, v in tokenized.items()}\n",
    "  tokenized_result = model.generate(**tokenized, max_length=128)\n",
    "  tokenized_result = tokenized_result.to('cpu')\n",
    "  predicted_summary = bert_tokenizer.decode(tokenized_result[0])\n",
    "  return predicted_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\generation\\utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "predicted_summary = predict_summary(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pemerintah memberikan tenggat 14 hari kepada para konglomerat penandatangan MSAA untuk menyerahkan aset. Jika mangkir, mereka bakal dihukum.'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "def get_rouge_scores(actual_summary, predicted_summary):\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(predicted_summary, actual_summary)\n",
    "    return [scores[0]['rouge-1']['f'], scores[0]['rouge-2']['f'], scores[0]['rouge-l']['f']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10972 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10972/10972 [1:14:36<00:00,  2.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# rouge score of validation data\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "rouge1_scores = []\n",
    "rouge2_scores = []\n",
    "rougel_scores = []\n",
    "\n",
    "pred_summary_list = []\n",
    "\n",
    "for i in tqdm(range(len(data_pt_test['clean_summary']))):\n",
    "\n",
    "  doc = data_pt_test['clean_article'][i]\n",
    "  pred_summary = predict_summary(doc)\n",
    "  human_summary = data_pt_test['clean_summary'][i]\n",
    "\n",
    "  score = get_rouge_scores(human_summary, pred_summary)\n",
    "\n",
    "  rouge1_scores.append(score[0])\n",
    "  rouge2_scores.append(score[1])\n",
    "  rougel_scores.append(score[2])\n",
    "\n",
    "  pred_summary_list.append(pred_summary)\n",
    "\n",
    "eval = pd.DataFrame({\n",
    "    \"pred_summary\" : pred_summary_list,\n",
    "    \"rouge1\" : rouge1_scores,\n",
    "    \"rouge2\" : rouge2_scores,\n",
    "    \"rougel\" : rougel_scores,\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average rouge 1\n",
    "eval['rouge1'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average rouge 2\n",
    "eval['rouge2'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average rouge l\n",
    "eval['rougel'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_summary</th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[CLS],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[CLS],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[CLS],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[CLS],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[CLS],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10967</th>\n",
       "      <td>[CLS],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10968</th>\n",
       "      <td>[CLS],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10969</th>\n",
       "      <td>[CLS],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10970</th>\n",
       "      <td>[CLS],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10971</th>\n",
       "      <td>[CLS],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10972 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            pred_summary  rouge1  rouge2  \\\n",
       "0      [CLS],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,...     0.0     0.0   \n",
       "1      [CLS],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,...     0.0     0.0   \n",
       "2      [CLS],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,...     0.0     0.0   \n",
       "3      [CLS],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,...     0.0     0.0   \n",
       "4      [CLS],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,...     0.0     0.0   \n",
       "...                                                  ...     ...     ...   \n",
       "10967  [CLS],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,...     0.0     0.0   \n",
       "10968  [CLS],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,...     0.0     0.0   \n",
       "10969  [CLS],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,...     0.0     0.0   \n",
       "10970  [CLS],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,...     0.0     0.0   \n",
       "10971  [CLS],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,...     0.0     0.0   \n",
       "\n",
       "       rougel  \n",
       "0         0.0  \n",
       "1         0.0  \n",
       "2         0.0  \n",
       "3         0.0  \n",
       "4         0.0  \n",
       "...       ...  \n",
       "10967     0.0  \n",
       "10968     0.0  \n",
       "10969     0.0  \n",
       "10970     0.0  \n",
       "10971     0.0  \n",
       "\n",
       "[10972 rows x 4 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
